{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw RAG 08: Evaluation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the development of Retrieval-Augmented Generation (RAG) systems, continuous improvement is essential. However, relying solely on intuition or \"vibe\" to assess performance enhancements can be misleading. This notebook introduces a systematic approach to evaluating RAG systems using key metrics inspired by the (RAGAS)[https://docs.ragas.io/en/latest/concepts/metrics/index.html] framework. Additionally, we'll implement these metrics in a Streamlit application, providing an interactive and user-friendly interface to visualize and analyze the results.\n",
    "\n",
    "### The Importance of Systematic Evaluation\n",
    "\n",
    "As we implement various enhancements in our RAG pipeline, it's crucial to have reliable methods to quantify improvements. Systematic evaluation allows us to:\n",
    "\n",
    "1. **Objectively Measure Performance**: Move beyond subjective assessments to concrete, comparable metrics.\n",
    "2. **Identify Strengths and Weaknesses**: Pinpoint specific areas where our system excels or needs improvement.\n",
    "3. **Guide Development Decisions**: Make informed choices about optimizations based on quantifiable results.\n",
    "4. **Ensure Consistency**: Maintain a standard benchmark to track progress over time and across different implementations.\n",
    "\n",
    "### Focus on Key Metrics\n",
    "\n",
    "In this notebook, we'll explore several critical evaluation metrics for RAG systems, including:\n",
    "\n",
    "1. **Faithfulness**: Assessing how accurately the generated response reflects the retrieved context.\n",
    "2. **Relevance**: Evaluating how well the response addresses the given query.\n",
    "3. **Context Precision**: Measuring the accuracy and relevance of the retrieved context.\n",
    "4. **Context Recall**: Evaluating how well the retrieved context covers the information needed to answer the query.\n",
    "5. **Context Entities Recall**: Assessing how effectively the system retrieves specific entities relevant to the query.\n",
    "\n",
    "These metrics, while inspired by the RAGAS framework, represent a focused approach to RAG evaluation that can be implemented without the full RAGAS toolkit.\n",
    "\n",
    "### LLM-as-Judge Approach: Considerations and Mitigation Strategies\n",
    "\n",
    "In this evaluation framework, we employ an LLM-as-judge approach for assessing our RAG system's performance. While this method allows for efficient processing of large volumes of data, it's important to acknowledge its limitations:\n",
    "\n",
    "1. **Potential Bias**: LLMs may have inherent biases that could affect their judgment.\n",
    "2. **Consistency Concerns**: LLMs might not always provide consistent evaluations across similar inputs.\n",
    "3. **Lack of Human Nuance**: LLMs may miss subtle context or nuances that a human evaluator would catch.\n",
    "\n",
    "To mitigate these limitations and enhance the accuracy of our evaluation, we implement the following strategies:\n",
    "\n",
    "1. **Detailed Evaluation Criteria**: We provide the LLM judge with comprehensive and specific criteria for each metric. This ensures a more structured and consistent evaluation process.\n",
    "\n",
    "2. **Example-based Instruction**: We include carefully crafted examples of good and poor performances for each metric. This helps calibrate the LLM's judgment and provides clear benchmarks.\n",
    "\n",
    "3. **Multiple Evaluations**: For critical or ambiguous cases, we may run multiple evaluations and aggregate the results to reduce inconsistency.\n",
    "\n",
    "4. **Human Oversight**: While the bulk of the evaluation is automated, we suggest periodic human reviews to validate the LLM's judgments and refine our evaluation process.\n",
    "\n",
    "5. **Transparency**: We clearly communicate that an LLM is used for evaluation, acknowledging both the benefits (scale, speed) and potential limitations of this approach.\n",
    "\n",
    "By implementing these strategies, we aim to leverage the efficiency of LLM-based evaluation while maintaining a high standard of accuracy and fairness in our assessment of the RAG system's performance.\n",
    "\n",
    "### Streamlit Implementation\n",
    "\n",
    "To enhance the accessibility and interactivity of our evaluation results, we'll implement a Streamlit application. This will allow users to:\n",
    "\n",
    "- Visualize metric scores through interactive charts and graphs\n",
    "- Compare performance across different RAG system configurations\n",
    "- Drill down into specific examples to understand metric calculations\n",
    "- Easily share and present evaluation results with stakeholders\n",
    "\n",
    "### What to Expect\n",
    "\n",
    "By the end of this notebook and accompanying Streamlit application, you'll understand:\n",
    "\n",
    "- How to calculate and interpret key evaluation metrics for RAG systems\n",
    "- Implementation techniques for these metrics in Python\n",
    "- How to use these metrics to guide your RAG system development\n",
    "- How to leverage Streamlit to create an interactive dashboard for RAG system evaluation\n",
    "\n",
    "While we're not implementing the full RAGAS framework, the concepts and techniques covered here provide a solid foundation for systematic RAG evaluation. This approach, combined with the Streamlit visualization, allows for a comprehensive and intuitive assessment of your RAG system's performance across various dimensions, from the relevance of retrieved information to the accuracy of generated responses.\n",
    "\n",
    "Let's dive in and explore how we can quantifiable assess, visualize, and improve our RAG implementations!\n",
    "\n",
    "**Note**: All the evaluation prompts are generated by Claude 3.5 Sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.35.10)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.37.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (5.4.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: packaging<25,>=20 in ./.venv/lib/python3.12/site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in ./.venv/lib/python3.12/site-packages (from streamlit) (4.25.3)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./.venv/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from streamlit) (8.5.0)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./.venv/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Downloading streamlit-1.37.0-py2.py3-none-any.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.9/311.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, tzdata, toolz, toml, smmap, rpds-py, pyarrow, blinker, referencing, pydeck, pandas, gitdb, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.3.0 blinker-1.8.2 gitdb-4.0.11 gitpython-3.1.43 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 pandas-2.2.2 pyarrow-17.0.0 pydeck-0.9.1 pytz-2024.1 referencing-0.35.1 rpds-py-0.19.1 smmap-5.0.1 streamlit-1.37.0 toml-0.10.2 toolz-0.12.1 tzdata-2024.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv streamlit pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables from the .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the path to your .env file if it's not in the same directory\n",
    "dotenv_path = \".env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_response(system_prompt, full_query):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": full_query},\n",
    "        ],\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the information needed for the LLM \"Judge\"\n",
    "\n",
    "query = \"How does climate change affect global sea levels, and what are some potential consequences?\"\n",
    "\n",
    "answer = \"Climate change is causing global sea levels to rise at an accelerating rate. This is primarily due to two factors: thermal expansion of the oceans as they warm and the melting of land-based ice, particularly glaciers and ice sheets in Greenland and Antarctica. The rate of sea-level rise has more than doubled from 1.4 mm per year throughout most of the 20th century to 3.6 mm per year from 2006-2015. Consequences of rising sea levels include increased coastal flooding, erosion of coastlines, saltwater intrusion into freshwater aquifers, and the potential displacement of millions of people living in low-lying coastal areas. Some island nations, like the Maldives, are at risk of becoming completely submerged. Additionally, rising seas can damage critical infrastructure and ecosystems, such as wetlands and mangrove forests, which provide natural protection against storms and serve as important habitats for many species.\"\n",
    "\n",
    "context = [\"Global mean sea level has risen about 8-9 inches (21-24 centimeters) since 1880, with about a third of that coming in just the last two and a half decades.\",\n",
    "\"The two major causes of global sea-level rise are thermal expansion caused by warming of the ocean and increased melting of land-based ice, such as glaciers and ice sheets.\",\n",
    "\"The oceans are absorbing more than 90 percent of the increased atmospheric heat associated with emissions from human activity.\",\n",
    "\"Scientists estimate that the global mean sea level could rise by 2-7 feet (0.6-2.1 meters) by 2100.\",\n",
    "\"Sea level rise poses threats to coastal communities, infrastructure, and ecosystems.\",\n",
    "\"Saltwater intrusion into freshwater aquifers can contaminate drinking water sources and affect agricultural production in coastal areas.\",\n",
    "\"The Intergovernmental Panel on Climate Change (IPCC) projects that climate change could displace millions of people due to coastal flooding by 2100.\",\n",
    "\"Some low-lying island nations, such as the Maldives and Marshall Islands, are particularly vulnerable to sea-level rise.\",\n",
    "\"Wetlands and mangrove forests along coastlines provide natural protection against storms and flooding.\",\n",
    "\"The rate of sea-level rise varies regionally due to factors such as local land subsidence, ocean currents, and variations in land height.\"]\n",
    "\n",
    "ground_truth = [\n",
    "    \"Climate change is causing global sea levels to rise at an accelerating rate.\",\n",
    "    \"The primary factors contributing to sea level rise are thermal expansion of warming oceans and melting of land-based ice, particularly glaciers and ice sheets in Greenland and Antarctica.\",\n",
    "    \"The rate of sea-level rise has more than doubled from 1.4 mm per year throughout most of the 20th century to 3.6 mm per year from 2006-2015.\"\n",
    "    \"Consequences of rising sea levels include increased coastal flooding, erosion of coastlines, and saltwater intrusion into freshwater aquifers.\",\n",
    "    \"Millions of people living in low-lying coastal areas are at risk of displacement due to sea level rise.\",\n",
    "    \"Some island nations, like the Maldives, are at risk of becoming completely submerged.\",\n",
    "    \"Rising seas can damage critical infrastructure and ecosystems, such as wetlands and mangrove forests.\",\n",
    "    \"Wetlands and mangrove forests provide natural protection against storms and serve as important habitats for many species.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faithfulness is a crucial metric in RAG systems that measures how accurately the generated response aligns with the information provided in the retrieved context. In other words, it evaluates whether the system's answer is truly based on the retrieved information rather than hallucinated or externally sourced content.\n",
    "\n",
    "Key aspects of faithfulness include:\n",
    "\n",
    "1. Factual Accuracy: The generated response should not contradict or misrepresent facts present in the retrieved context.\n",
    "2. Information Source: All key information in the response should be traceable back to the retrieved context.\n",
    "3. Avoidance of Hallucination: The system should not introduce new information that isn't supported by the context.\n",
    "4. Appropriate Uncertainty: When the context doesn't provide complete information, the response should reflect this uncertainty rather than making unfounded claims.\n",
    "\n",
    "Evaluating faithfulness typically involves comparing the generated response against the retrieved context, often using techniques like natural language inference or carefully designed prompts for LLM-based judges.\n",
    "\n",
    "High faithfulness is essential for building trust in RAG systems, as it ensures that the system's outputs are grounded in the provided information rather than being fabricated or misleading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faithfulness Calculation Prompt\n",
    "faithfulness_prompt = f\"\"\"\n",
    "You are an impartial judge tasked with evaluating the faithfulness of an answer to a given question based on the provided context. Your goal is to determine how well the answer is supported by the context and calculate a faithfulness score using the following formula:\n",
    "\n",
    "Faithfulness score = (Number of claims in the generated answer that can be inferred from given context) / (Total number of claims in the generated answer)\n",
    "\n",
    "Given:\n",
    "- Question: {query}\n",
    "- Answer: {answer}\n",
    "- Context: {context}\n",
    "\n",
    "Please follow these steps:\n",
    "\n",
    "1. Identify claims in the answer:\n",
    "   a. Break down the answer into individual claims or statements.\n",
    "   b. List each claim separately.\n",
    "   c. Count the total number of claims.\n",
    "\n",
    "2. Analyze each claim:\n",
    "   a. For each claim, determine if it can be inferred from the given context.\n",
    "   b. Provide a brief explanation for your decision on each claim.\n",
    "   c. Count the number of claims that can be inferred from the context.\n",
    "\n",
    "3. Calculate the faithfulness score:\n",
    "   a. Use the formula: (Number of claims inferred from context) / (Total number of claims)\n",
    "   b. Express the result as a decimal between 0 and 1.\n",
    "\n",
    "4. Provide a summary:\n",
    "   a. State the faithfulness score.\n",
    "   b. Summarize your evaluation, including:\n",
    "      - Total number of claims in the answer\n",
    "      - Number of claims that can be inferred from the context\n",
    "      - Any notable observations about the answer's faithfulness to the context\n",
    "   c. If applicable, highlight any claims in the answer that are not supported by the context.\n",
    "\n",
    "Please present your evaluation in a clear, structured format, showing your work for each step of the process.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "1. Claims Identification:\n",
    "   - Claim 1: [Text of claim]\n",
    "   - Claim 2: [Text of claim]\n",
    "   ...\n",
    "   Total claims: [Number]\n",
    "\n",
    "2. Claims Analysis:\n",
    "   - Claim 1: [Can/Cannot be inferred] - [Brief explanation]\n",
    "   - Claim 2: [Can/Cannot be inferred] - [Brief explanation]\n",
    "   ...\n",
    "   Claims inferred from context: [Number]\n",
    "\n",
    "3. Faithfulness Score Calculation:\n",
    "   [Number of inferred claims] / [Total claims] = [Score]\n",
    "\n",
    "4. Summary:\n",
    "   Faithfulness Score: \n",
    "   <score>\n",
    "   [Score]\n",
    "   </score>\n",
    "   [Summary text]\n",
    "   [Unsupported claims, if any]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Claims Identification:\n",
      "   - Claim 1: Climate change is causing global sea levels to rise at an accelerating rate.\n",
      "   - Claim 2: This is primarily due to two factors: thermal expansion of the oceans as they warm and the melting of land-based ice, particularly glaciers and ice sheets in Greenland and Antarctica.\n",
      "   - Claim 3: The rate of sea-level rise has more than doubled from 1.4 mm per year throughout most of the 20th century to 3.6 mm per year from 2006-2015.\n",
      "   - Claim 4: Consequences of rising sea levels include increased coastal flooding, erosion of coastlines, saltwater intrusion into freshwater aquifers, and the potential displacement of millions of people living in low-lying coastal areas.\n",
      "   - Claim 5: Some island nations, like the Maldives, are at risk of becoming completely submerged.\n",
      "   - Claim 6: Rising seas can damage critical infrastructure and ecosystems, such as wetlands and mangrove forests, which provide natural protection against storms and serve as important habitats for many species.\n",
      "   Total claims: 6\n",
      "\n",
      "2. Claims Analysis:\n",
      "   - Claim 1: Can be inferred - The context mentions that global mean sea level has risen and continues to rise, implying an accelerating rate.\n",
      "   - Claim 2: Can be inferred - The context explicitly states the two major causes of sea-level rise as thermal expansion and increased melting of land-based ice.\n",
      "   - Claim 3: Cannot be inferred - The specific rates of sea-level rise mentioned are not provided in the context.\n",
      "   - Claim 4: Can be inferred - The context discusses threats to coastal communities, infrastructure, and ecosystems, and mentions saltwater intrusion and potential displacement due to coastal flooding.\n",
      "   - Claim 5: Can be inferred - The context specifically mentions that low-lying island nations like the Maldives are vulnerable to sea-level rise.\n",
      "   - Claim 6: Can be inferred - The context mentions that wetlands and mangrove forests provide natural protection against storms and flooding, implying damage to these areas from rising seas.\n",
      "   Claims inferred from context: 5\n",
      "\n",
      "3. Faithfulness Score Calculation:\n",
      "   5 / 6 = 0.833\n",
      "\n",
      "4. Summary:\n",
      "   Faithfulness Score: 0.833\n",
      "   The answer is largely faithful to the provided context, with 5 out of 6 claims being supported. The only unsupported claim is the specific rates of sea-level rise from 1.4 mm per year to 3.6 mm per year, which are not mentioned in the context. The rest of the answer aligns well with the context, discussing the causes, consequences, and specific risks associated with rising sea levels.\n"
     ]
    }
   ],
   "source": [
    "faithfulness_system_prompt = (\n",
    "    \"You are an impartial judge evaluating the faithfulness of an answer to a question based on given context. Analyze the answer's claims, determine if each claim can be inferred from the context, and calculate a faithfulness score. Be objective and thorough in your assessment.\"\n",
    ")\n",
    "\n",
    "faithfulness_response = generate_response(\n",
    "    faithfulness_system_prompt, faithfulness_prompt\n",
    ")\n",
    "\n",
    "print(faithfulness_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance Calculation\n",
    "\n",
    "Relevance measures how well the generated response addresses the user's query. Calculating relevance involves:\n",
    "\n",
    "1. **Query-Response Alignment**: Assessing how directly the response answers the question posed.\n",
    "2. **Information Pertinence**: Evaluating whether the response contains information that is actually useful for the query.\n",
    "3. **Semantic Similarity**: Measuring the topical closeness between the query and the response.\n",
    "4. **Conciseness vs. Comprehensiveness**: Balancing providing enough information without irrelevant details.\n",
    "\n",
    "Relevance calculation often employs techniques such as semantic similarity metrics, supervised learning models trained on human-labeled data, or carefully prompt-engineered language models to assess the quality of the response in relation to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Relevancy Calculation Prompt\n",
    "\n",
    "relevance_prompt = f\"\"\"\n",
    "You are an AI assistant tasked with evaluating the relevancy of an answer to a given question. Your goal is to calculate an Answer Relevancy score using the following process:\n",
    "\n",
    "1. Given:\n",
    "   - Original Question: {query}\n",
    "   - Generated Answer: {answer}\n",
    "   - Context (if provided): {context}\n",
    "\n",
    "2. Task Overview:\n",
    "   Your task is to generate artificial questions based on the answer, compare these to the original question, and calculate a relevancy score.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "   a. Generate Questions:\n",
    "      - Create 3 artificial questions that the given answer could be responding to.\n",
    "      - These questions should be diverse and cover different aspects of the answer.\n",
    "      - List these questions clearly.\n",
    "\n",
    "   b. Conceptual Embedding and Similarity:\n",
    "      - For each generated question and the original question, imagine you are creating a semantic embedding. This embedding would represent the meaning of the question in a high-dimensional space.\n",
    "      - Conceptually compare each generated question to the original question. Consider how similar they are in meaning and intent.\n",
    "      - Assign a similarity score between -1 and 1 for each comparison, where:\n",
    "        * 1 indicates perfect similarity\n",
    "        * 0 indicates no relation\n",
    "        * -1 indicates opposite meanings\n",
    "      - Explain your reasoning for each similarity score.\n",
    "\n",
    "   c. Calculate Answer Relevancy:\n",
    "      - Use the formula: Answer Relevancy = (Sum of similarity scores) / (Number of generated questions)\n",
    "      - Show your calculation.\n",
    "\n",
    "4. Final Output:\n",
    "   - List the original question and generated questions\n",
    "   - Show similarity scores and explanations\n",
    "   - Present the final Answer Relevancy score\n",
    "   - Provide a brief interpretation of the score\n",
    "\n",
    "Remember, while you're conceptualizing embeddings and similarity, you're not actually generating numerical embeddings. Use your understanding of language and context to estimate similarity.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "Original Question: [Original question text]\n",
    "\n",
    "Generated Questions:\n",
    "1. [Question 1]\n",
    "2. [Question 2]\n",
    "3. [Question 3]\n",
    "\n",
    "Similarity Scores:\n",
    "1. Score: [X.XX] - Explanation: [Your reasoning]\n",
    "2. Score: [X.XX] - Explanation: [Your reasoning]\n",
    "3. Score: [X.XX] - Explanation: [Your reasoning]\n",
    "\n",
    "Calculation:\n",
    "Answer Relevancy = (Score1 + Score2 + Score3) / 3 = <final_score>[Final Score]</final_score>\n",
    "\n",
    "Interpretation:\n",
    "[Brief interpretation of the score and what it means for the answer's relevancy]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: How does climate change affect global sea levels, and what are some potential consequences?\n",
      "\n",
      "Generated Questions:\n",
      "1. What are the primary causes of rising sea levels and how have they changed over time?\n",
      "2. What impact does sea level rise have on coastal ecosystems and human populations?\n",
      "3. How have global sea levels changed since the 20th century, and what predictions exist for future levels?\n",
      "\n",
      "Similarity Scores:\n",
      "1. Score: 0.8 - Explanation: This question directly addresses the causes of rising sea levels, which is a central aspect of the original question. It slightly deviates by focusing more on historical changes rather than future predictions or broader consequences.\n",
      "2. Score: 0.9 - Explanation: This question closely aligns with the original question by exploring the consequences of rising sea levels, specifically focusing on ecosystems and human populations, which are mentioned as part of the consequences in the original question.\n",
      "3. Score: 0.7 - Explanation: This question addresses the historical and future changes in sea levels, which is relevant to the original question's focus on the effects of climate change on sea levels. However, it does not directly address the broader consequences, focusing instead on the measurement and prediction aspects.\n",
      "\n",
      "Calculation:\n",
      "Answer Relevancy = (0.8 + 0.9 + 0.7) / 3 = 0.8\n",
      "\n",
      "Interpretation:\n",
      "The final Answer Relevancy score of 0.8 indicates a high degree of relevance between the generated answer and the original question. The answer effectively addresses the causes and consequences of rising sea levels due to climate change, as queried in the original question. The slight variations in the generated questions focus on specific aspects of the broader topic, maintaining a strong overall relevancy.\n"
     ]
    }
   ],
   "source": [
    "relevance_system_prompt = \"You are an AI assistant specialized in evaluating the relevancy of answers to questions. Your task is to generate artificial questions based on a given answer, compare them to the original question, and calculate an Answer Relevancy score. Use your language understanding to conceptualize semantic similarities without performing actual mathematical calculations.\"\n",
    "\n",
    "relevance_response = generate_response(relevance_system_prompt, relevance_prompt)\n",
    "\n",
    "print(relevance_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Precision\n",
    "\n",
    "Context precision evaluates the accuracy and relevance of the retrieved context used to generate the response. Key aspects include:\n",
    "\n",
    "1. **Information Density**: Assessing how much of the retrieved context is actually relevant to the query.\n",
    "2. **Noise Reduction**: Measuring the system's ability to filter out irrelevant or redundant information.\n",
    "3. **Conciseness**: Evaluating whether the context contains only necessary information without extraneous details.\n",
    "4. **Query Alignment**: Determining how well the retrieved context aligns with the specific requirements of the query.\n",
    "\n",
    "Calculating context precision often involves comparing the retrieved context against ideal or human-curated responses, using techniques such as content overlap analysis, semantic similarity measures, or machine learning models trained on annotated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Precision Calculation Prompt\n",
    "\n",
    "context_precision_prompt = f\"\"\"\n",
    "You are an AI assistant tasked with evaluating the precision of context chunks provided for a given question. Your goal is to calculate a Context Precision score using the following process:\n",
    "\n",
    "1. Given:\n",
    "   - Question: {query}\n",
    "   - Ground Truth: {answer}\n",
    "   - Contexts: {context}\n",
    "\n",
    "2. Task Overview:\n",
    "   Your task is to evaluate the relevance of each context chunk, calculate precision at each rank, and then compute the overall Context Precision score.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "   a. Evaluate Relevance:\n",
    "      - For each context chunk, determine if it's relevant to answering the question based on the ground truth.\n",
    "      - Assign a relevance indicator (v_k) of 1 if relevant, 0 if not relevant.\n",
    "      - List your decisions and briefly explain your reasoning for each.\n",
    "\n",
    "   b. Calculate Precision at each rank (Precision@k):\n",
    "      - For each rank k from 1 to K:\n",
    "        * Count the number of relevant items up to and including rank k (true positives).\n",
    "        * Calculate Precision@k = (true positives at k) / k\n",
    "      - Show your calculations for each k.\n",
    "\n",
    "   c. Calculate Context Precision@K:\n",
    "      - Sum the products of (Precision@k * v_k) for all k from 1 to K.\n",
    "      - Divide this sum by the total number of relevant items in the top K results.\n",
    "      - Show your calculation.\n",
    "\n",
    "4. Final Output:\n",
    "   - List the relevance decisions for each context chunk\n",
    "   - Show Precision@k calculations for each k\n",
    "   - Present the final Context Precision@K score\n",
    "   - Provide a brief interpretation of the score\n",
    "\n",
    "Remember to be objective in your relevance assessments and precise in your calculations.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "Question: [Question text]\n",
    "Ground Truth: [Ground truth text]\n",
    "\n",
    "Relevance Evaluations:\n",
    "1. Context Chunk 1: [Relevant/Not Relevant] - v_1 = [0/1] - Explanation: [Brief reasoning]\n",
    "2. Context Chunk 2: [Relevant/Not Relevant] - v_2 = [0/1] - Explanation: [Brief reasoning]\n",
    "...\n",
    "K. Context Chunk K: [Relevant/Not Relevant] - v_K = [0/1] - Explanation: [Brief reasoning]\n",
    "\n",
    "Precision@k Calculations:\n",
    "Precision@1 = [calculation] = [result]\n",
    "Precision@2 = [calculation] = [result]\n",
    "...\n",
    "Precision@K = [calculation] = [result]\n",
    "\n",
    "Context Precision@K Calculation:\n",
    "Sum of (Precision@k * v_k) = [calculation]\n",
    "Total number of relevant items = [number]\n",
    "Context Precision@K = [final calculation] = [Final Score]\n",
    "\n",
    "Interpretation:\n",
    "[Brief interpretation of the score and what it means for the context precision]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does climate change affect global sea levels, and what are some potential consequences?\n",
      "Ground Truth: Climate change is causing global sea levels to rise at an accelerating rate. This is primarily due to two factors: thermal expansion of the oceans as they warm and the melting of land-based ice, particularly glaciers and ice sheets in Greenland and Antarctica. The rate of sea-level rise has more than doubled from 1.4 mm per year throughout most of the 20th century to 3.6 mm per year from 2006-2015. Consequences of rising sea levels include increased coastal flooding, erosion of coastlines, saltwater intrusion into freshwater aquifers, and the potential displacement of millions of people living in low-lying coastal areas. Some island nations, like the Maldives, are at risk of becoming completely submerged. Additionally, rising seas can damage critical infrastructure and ecosystems, such as wetlands and mangrove forests, which provide natural protection against storms and serve as important habitats for many species.\n",
      "\n",
      "Relevance Evaluations:\n",
      "1. Context Chunk 1: Relevant - v_1 = 1 - Explanation: Mentions the historical rise in sea levels, which is directly related to the question.\n",
      "2. Context Chunk 2: Relevant - v_2 = 1 - Explanation: Directly addresses the causes of sea-level rise mentioned in the ground truth.\n",
      "3. Context Chunk 3: Not Relevant - v_3 = 0 - Explanation: Discusses ocean heat absorption but does not directly relate to sea-level rise or its consequences.\n",
      "4. Context Chunk 4: Relevant - v_4 = 1 - Explanation: Provides future projections of sea-level rise, relevant to understanding potential future impacts.\n",
      "5. Context Chunk 5: Relevant - v_5 = 1 - Explanation: Discusses threats to coastal communities, directly addressing the consequences of sea-level rise.\n",
      "6. Context Chunk 6: Relevant - v_6 = 1 - Explanation: Saltwater intrusion is a specific consequence of sea-level rise mentioned in the ground truth.\n",
      "7. Context Chunk 7: Relevant - v_7 = 1 - Explanation: Addresses displacement due to coastal flooding, a consequence of sea-level rise.\n",
      "8. Context Chunk 8: Relevant - v_8 = 1 - Explanation: Specific example of vulnerable nations, aligns with consequences mentioned in the ground truth.\n",
      "9. Context Chunk 9: Relevant - v_9 = 1 - Explanation: Discusses the role of wetlands and mangroves in storm protection, relevant to consequences of sea-level rise.\n",
      "10. Context Chunk 10: Not Relevant - v_10 = 0 - Explanation: Discusses regional variations in sea-level rise but does not directly address the global impact or specific consequences.\n",
      "\n",
      "Precision@k Calculations:\n",
      "Precision@1 = 1/1 = 1.00\n",
      "Precision@2 = 2/2 = 1.00\n",
      "Precision@3 = 2/3 ≈ 0.67\n",
      "Precision@4 = 3/4 = 0.75\n",
      "Precision@5 = 4/5 = 0.80\n",
      "Precision@6 = 5/6 ≈ 0.83\n",
      "Precision@7 = 6/7 ≈ 0.86\n",
      "Precision@8 = 7/8 = 0.88\n",
      "Precision@9 = 8/9 ≈ 0.89\n",
      "Precision@10 = 8/10 = 0.80\n",
      "\n",
      "Context Precision@K Calculation:\n",
      "Sum of (Precision@k * v_k) = 1*1 + 1*1 + 0.67*0 + 0.75*1 + 0.80*1 + 0.83*1 + 0.86*1 + 0.88*1 + 0.89*1 + 0.80*0 = 7.01\n",
      "Total number of relevant items = 8\n",
      "Context Precision@K = 7.01 / 8 ≈ 0.88\n",
      "\n",
      "Interpretation:\n",
      "The Context Precision@K score of approximately 0.88 indicates a high level of precision in the context chunks provided relative to the question and ground truth. Most context chunks directly address the causes and consequences of sea-level rise due to climate change, demonstrating their relevance and utility in answering the question.\n"
     ]
    }
   ],
   "source": [
    "context_precision_system_prompt = \"You are an AI assistant specialized in evaluating the precision of context chunks for given questions. Your task is to assess the relevance of each context chunk, calculate precision at various ranks, and compute an overall Context Precision score. Use your understanding of the question, ground truth, and contexts to make objective assessments and perform accurate calculations.\"\n",
    "\n",
    "context_precision_response = generate_response(\n",
    "    context_precision_system_prompt, context_precision_prompt\n",
    ")\n",
    "\n",
    "print(context_precision_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Relevancy Calculation\n",
    "\n",
    "Context Relevancy measures how well the retrieved context aligns with the user's query. This metric is crucial for ensuring that the RAG system is working with pertinent information. Key aspects include:\n",
    "\n",
    "1. **Query-Context Alignment**: Assessing how closely the retrieved context matches the information needs expressed in the query.\n",
    "2. **Semantic Overlap**: Measuring the topical similarity between the query and the retrieved context.\n",
    "3. **Information Sufficiency**: Evaluating whether the context contains enough relevant information to adequately answer the query.\n",
    "4. **Contextual Appropriateness**: Determining if the retrieved context is suitable for the query's domain and complexity level.\n",
    "\n",
    "Calculating context relevancy often involves techniques such as cosine similarity between query and context embeddings, supervised machine learning models trained on human-annotated data, or carefully designed prompts for LLM-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Relevancy Calculation Prompt\n",
    "\n",
    "context_relevance_prompt = f\"\"\"\n",
    "You are an AI assistant tasked with evaluating the relevancy of retrieved context for a given question. Your goal is to calculate a Context Relevancy score using the following process:\n",
    "\n",
    "1. Given:\n",
    "   - Question: {query}\n",
    "   - Retrieved Context: {context}\n",
    "\n",
    "2. Task Overview:\n",
    "   Your task is to identify relevant sentences in the retrieved context, count them, and calculate the Context Relevancy score.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "   a. Sentence Identification:\n",
    "      - Break down the retrieved context into individual sentences.\n",
    "      - Number each sentence for easy reference.\n",
    "\n",
    "   b. Relevance Evaluation:\n",
    "      - For each sentence, determine if it's relevant to answering the question.\n",
    "      - Assign a relevance indicator of 1 if relevant, 0 if not relevant.\n",
    "      - Briefly explain your reasoning for each decision.\n",
    "\n",
    "   c. Calculate Context Relevancy:\n",
    "      - Count the total number of sentences in the retrieved context.\n",
    "      - Count the number of relevant sentences (|S|).\n",
    "      - Calculate the Context Relevancy score using the formula:\n",
    "        Context Relevancy = |S| / (Total number of sentences in retrieved context)\n",
    "      - Show your calculation.\n",
    "\n",
    "4. Final Output:\n",
    "   - List all sentences with their relevance decisions\n",
    "   - Show the calculation of the Context Relevancy score\n",
    "   - Present the final Context Relevancy score\n",
    "   - Provide a brief interpretation of the score\n",
    "\n",
    "Remember to be objective in your relevance assessments and precise in your calculations.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "Question: [Question text]\n",
    "\n",
    "Sentence Evaluation:\n",
    "1. [Sentence 1]: [Relevant/Not Relevant] - Explanation: [Brief reasoning]\n",
    "2. [Sentence 2]: [Relevant/Not Relevant] - Explanation: [Brief reasoning]\n",
    "...\n",
    "N. [Sentence N]: [Relevant/Not Relevant] - Explanation: [Brief reasoning]\n",
    "\n",
    "Calculation:\n",
    "Total number of sentences: [N]\n",
    "Number of relevant sentences (|S|): [X]\n",
    "Context Relevancy = X / N = [Final Score]\n",
    "\n",
    "Interpretation:\n",
    "[Brief interpretation of the score and what it means for the context relevancy]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does climate change affect global sea levels, and what are some potential consequences?\n",
      "\n",
      "Sentence Evaluation:\n",
      "1. Global mean sea level has risen about 8-9 inches (21-24 centimeters) since 1880, with about a third of that coming in just the last two and a half decades.: Relevant - Explains the historical rise in sea levels, which is a direct consequence of climate change.\n",
      "2. The two major causes of global sea-level rise are thermal expansion caused by warming of the ocean and increased melting of land-based ice, such as glaciers and ice sheets.: Relevant - Identifies the causes of sea level rise related to climate change.\n",
      "3. The oceans are absorbing more than 90 percent of the increased atmospheric heat associated with emissions from human activity.: Relevant - Provides information on how the oceans' absorption of heat (related to climate change) contributes to thermal expansion and sea level rise.\n",
      "4. Scientists estimate that the global mean sea level could rise by 2-7 feet (0.6-2.1 meters) by 2100.: Relevant - Offers projections of future sea level rise due to climate change.\n",
      "5. Sea level rise poses threats to coastal communities, infrastructure, and ecosystems.: Relevant - Discusses potential consequences of sea level rise, addressing the second part of the question.\n",
      "6. Saltwater intrusion into freshwater aquifers can contaminate drinking water sources and affect agricultural production in coastal areas.: Relevant - Explains a specific consequence of sea level rise, which is relevant to the question's focus on potential consequences.\n",
      "7. The Intergovernmental Panel on Climate Change (IPCC) projects that climate change could displace millions of people due to coastal flooding by 2100.: Relevant - Highlights a significant human impact of sea level rise, aligning with the question's focus on consequences.\n",
      "8. Some low-lying island nations, such as the Maldives and Marshall Islands, are particularly vulnerable to sea-level rise.: Relevant - Provides examples of areas especially at risk, which underscores the consequences of sea level rise.\n",
      "9. Wetlands and mangrove forests along coastlines provide natural protection against storms and flooding.: Not Relevant - While informative about natural protections, it does not directly address how climate change affects sea levels or the specific consequences thereof.\n",
      "10. The rate of sea-level rise varies regionally due to factors such as local land subsidence, ocean currents, and variations in land height.: Relevant - Adds context to the variability of sea level rise impacts, which is useful for understanding regional differences in consequences.\n",
      "\n",
      "Calculation:\n",
      "Total number of sentences: 10\n",
      "Number of relevant sentences (|S|): 9\n",
      "Context Relevancy = 9 / 10 = 0.9\n",
      "\n",
      "Interpretation:\n",
      "The Context Relevancy score of 0.9 indicates a high degree of relevance between the retrieved context and the question. Most sentences directly address the causes and consequences of sea level changes due to climate change, making the context highly pertinent to the question. Only one sentence was deemed not directly relevant, as it did not specifically address the question's focus on climate change's effects or consequences.\n"
     ]
    }
   ],
   "source": [
    "context_relevance_system_prompt = \"You are an AI assistant specialized in evaluating the relevancy of retrieved context for given questions. Your task is to analyze individual sentences, determine their relevance to the question, and compute an overall Context Relevancy score. Use your understanding of the question and context to make objective assessments and perform accurate calculations.\"\n",
    "\n",
    "context_relevance_response = generate_response(\n",
    "    context_relevance_system_prompt, context_relevance_prompt\n",
    ")\n",
    "\n",
    "print(context_relevance_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Recall Calculation\n",
    "\n",
    "Context Recall measures how completely the retrieved context covers the information necessary to answer the query. This metric ensures that the RAG system isn't missing crucial information. Key aspects include:\n",
    "\n",
    "1. **Information Completeness**: Assessing whether all essential information required to answer the query is present in the retrieved context.\n",
    "2. **Coverage of Query Aspects**: Evaluating how well the context addresses all aspects or sub-questions within the main query.\n",
    "3. **Absence of Critical Gaps**: Identifying whether there are any significant information gaps that could lead to incomplete or misleading responses.\n",
    "4. **Breadth vs. Depth Balance**: Determining if the context provides a good balance between broad coverage and necessary depth on specific points.\n",
    "\n",
    "Calculating context recall often involves comparing the retrieved context against ideal or comprehensive reference answers, using techniques such as named entity recognition, key information extraction, or semantic similarity analysis between the context and a set of expected information points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Recall Calculation Prompt\n",
    "\n",
    "context_recall_prompt = f\"\"\"\n",
    "You are an AI assistant tasked with evaluating the recall of retrieved context compared to a ground truth answer. Your goal is to calculate a Context Recall score using the following process:\n",
    "\n",
    "1. Given:\n",
    "   - Ground Truth Answer: {answer}\n",
    "   - Retrieved Context: {context}\n",
    "\n",
    "2. Task Overview:\n",
    "   Your task is to analyze each sentence in the ground truth answer, determine if it can be attributed to the retrieved context, and calculate the Context Recall score.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "   a. Ground Truth Sentence Identification:\n",
    "      - Break down the ground truth answer into individual sentences.\n",
    "      - Number each sentence for easy reference.\n",
    "\n",
    "   b. Attribution Evaluation:\n",
    "      - For each ground truth sentence, determine if it can be attributed to (found in or inferred from) the retrieved context.\n",
    "      - Assign an attribution indicator of 1 if attributable, 0 if not attributable.\n",
    "      - Briefly explain your reasoning for each decision.\n",
    "\n",
    "   c. Calculate Context Recall:\n",
    "      - Count the total number of sentences in the ground truth answer.\n",
    "      - Count the number of ground truth sentences that can be attributed to the context.\n",
    "      - Calculate the Context Recall score using the formula:\n",
    "        Context Recall = (Number of GT sentences attributed to context) / (Total number of sentences in GT)\n",
    "      - Show your calculation.\n",
    "\n",
    "4. Final Output:\n",
    "   - List all ground truth sentences with their attribution decisions\n",
    "   - Show the calculation of the Context Recall score\n",
    "   - Present the final Context Recall score\n",
    "   - Provide a brief interpretation of the score\n",
    "\n",
    "Remember to be objective in your attribution assessments and precise in your calculations.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "Ground Truth Sentence Evaluation:\n",
    "1. [GT Sentence 1]: [Attributable/Not Attributable] - Explanation: [Brief reasoning]\n",
    "2. [GT Sentence 2]: [Attributable/Not Attributable] - Explanation: [Brief reasoning]\n",
    "...\n",
    "N. [GT Sentence N]: [Attributable/Not Attributable] - Explanation: [Brief reasoning]\n",
    "\n",
    "Calculation:\n",
    "Total number of GT sentences: [N]\n",
    "Number of GT sentences attributable to context: [X]\n",
    "Context Recall = X / N = [Final Score]\n",
    "\n",
    "Interpretation:\n",
    "[Brief interpretation of the score and what it means for the context recall]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Sentence Evaluation:\n",
      "1. Climate change is causing global sea levels to rise at an accelerating rate. [Attributable] - Explanation: The retrieved context mentions that sea level has risen significantly since 1880, with a substantial increase in the last few decades, implying an accelerating rate.\n",
      "2. This is primarily due to two factors: thermal expansion of the oceans as they warm and the melting of land-based ice, particularly glaciers and ice sheets in Greenland and Antarctica. [Attributable] - Explanation: The retrieved context explicitly states the two major causes of sea-level rise as thermal expansion and increased melting of land-based ice.\n",
      "3. The rate of sea-level rise has more than doubled from 1.4 mm per year throughout most of the 20th century to 3.6 mm per year from 2006-2015. [Not Attributable] - Explanation: The retrieved context does not provide specific historical rates or comparisons of sea-level rise rates over these exact time frames.\n",
      "4. Consequences of rising sea levels include increased coastal flooding, erosion of coastlines, saltwater intrusion into freshwater aquifers, and the potential displacement of millions of people living in low-lying coastal areas. [Attributable] - Explanation: The retrieved context mentions threats to coastal communities, infrastructure, ecosystems, and displacement of millions due to coastal flooding, which covers most of these consequences.\n",
      "5. Some island nations, like the Maldives, are at risk of becoming completely submerged. [Attributable] - Explanation: The retrieved context specifically mentions the Maldives as being vulnerable to sea-level rise.\n",
      "6. Additionally, rising seas can damage critical infrastructure and ecosystems, such as wetlands and mangrove forests, which provide natural protection against storms and serve as important habitats for many species. [Attributable] - Explanation: The retrieved context mentions that wetlands and mangrove forests provide natural protection against storms and flooding, aligning with the statement about damage to ecosystems and infrastructure.\n",
      "\n",
      "Calculation:\n",
      "Total number of GT sentences: 6\n",
      "Number of GT sentences attributable to context: 5\n",
      "Context Recall = 5 / 6 = 0.8333\n",
      "\n",
      "Interpretation:\n",
      "The Context Recall score of 0.8333 indicates a high level of recall, with most of the ground truth information being covered or inferred from the retrieved context. This suggests that the retrieved context is quite comprehensive in addressing the key aspects of sea-level rise due to climate change, although it lacks specific historical rate data.\n"
     ]
    }
   ],
   "source": [
    "context_recall_system_prompt = \"You are an AI assistant specialized in evaluating the recall of retrieved context compared to ground truth answers. Your task is to analyze individual sentences from the ground truth, determine their attribution to the retrieved context, and compute an overall Context Recall score. Use your understanding of the ground truth and context to make objective assessments and perform accurate calculations.\"\n",
    "\n",
    "context_recall_response = generate_response(\n",
    "    context_recall_system_prompt, context_recall_prompt\n",
    ")\n",
    "\n",
    "print(context_recall_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Entities Recall Calculation\n",
    "\n",
    "Context Entities Recall is a metric that evaluates how well the retrieved context captures the essential entities present in the ground truth. This metric is crucial for assessing the completeness and accuracy of the information retrieved by the RAG system.\n",
    "\n",
    "The calculation process typically involves:\n",
    "\n",
    "1. Entity Identification: \n",
    "   - Identify all entities in the ground truth.\n",
    "   - Identify all entities in the retrieved context.\n",
    "   - Entities may include named individuals, organizations, locations, dates, numerical facts, and other specific, identifiable information.\n",
    "\n",
    "2. Set Comparison:\n",
    "   - Determine the overlap between entities found in the ground truth and those in the retrieved context.\n",
    "\n",
    "3. Score Calculation:\n",
    "   - Calculate the Context Entities Recall score using the formula:\n",
    "     Context Entities Recall = |GE ∩ CE| / |GE|\n",
    "     Where:\n",
    "     - GE is the set of entities in the ground truth\n",
    "     - CE is the set of entities in the retrieved context\n",
    "     - |GE ∩ CE| is the number of entities common to both sets\n",
    "     - |GE| is the total number of entities in the ground truth\n",
    "\n",
    "4. Interpretation:\n",
    "   - Provide a meaningful interpretation of the calculated score, indicating how well the retrieved context captures the essential entities from the ground truth.\n",
    "\n",
    "This metric helps in quantifying the RAG system's ability to retrieve contextual information that contains the key entities necessary for answering queries accurately and comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Entities Recall Calculation Prompt\n",
    "\n",
    "context_entities_recall_prompt = f\"\"\"\n",
    "You are an AI assistant tasked with evaluating the recall of entities in retrieved context compared to ground truth. Your goal is to calculate a Context Entities Recall score using the following process:\n",
    "\n",
    "1. Given:\n",
    "   - Ground Truth: {ground_truth}\n",
    "   - Retrieved Context: {context}\n",
    "\n",
    "2. Task Overview:\n",
    "   Your task is to identify entities in both the ground truth and the retrieved context, compare these sets, and calculate the Context Entities Recall score.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "   a. Entity Identification:\n",
    "      - Identify all entities in the ground truth. List them as set GE.\n",
    "      - Identify all entities in the retrieved context. List them as set CE.\n",
    "      - Entities may include named individuals, organizations, locations, dates, numerical facts, and other specific, identifiable information.\n",
    "\n",
    "   b. Set Comparison:\n",
    "      - Identify the entities that appear in both GE and CE (the intersection).\n",
    "      - List these common entities.\n",
    "\n",
    "   c. Calculate Context Entities Recall:\n",
    "      - Count the number of entities in GE.\n",
    "      - Count the number of entities in the intersection of GE and CE.\n",
    "      - Calculate the Context Entities Recall score using the formula:\n",
    "        Context Entities Recall = |GE ∩ CE| / |GE|\n",
    "        (Where |GE ∩ CE| is the number of entities in the intersection, and |GE| is the total number of entities in the ground truth)\n",
    "      - Show your calculation.\n",
    "\n",
    "4. Final Output:\n",
    "   - List all entities found in the ground truth (GE)\n",
    "   - List all entities found in the retrieved context (CE)\n",
    "   - List the entities common to both (GE ∩ CE)\n",
    "   - Show the calculation of the Context Entities Recall score\n",
    "   - Present the final Context Entities Recall score\n",
    "   - Provide a brief interpretation of the score\n",
    "\n",
    "Remember to be thorough in your entity identification and precise in your calculations.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "Entities in Ground Truth (GE):\n",
    "[List of entities]\n",
    "\n",
    "Entities in Retrieved Context (CE):\n",
    "[List of entities]\n",
    "\n",
    "Common Entities (GE ∩ CE):\n",
    "[List of common entities]\n",
    "\n",
    "Calculation:\n",
    "|GE| (Total entities in ground truth): [Number]\n",
    "|GE ∩ CE| (Common entities): [Number]\n",
    "Context Entities Recall = |GE ∩ CE| / |GE| = [Final Score]\n",
    "\n",
    "Interpretation:\n",
    "[Brief interpretation of the score and what it means for the context entities recall]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Entities in Ground Truth (GE):\n",
      "- Climate change\n",
      "- Global sea levels\n",
      "- Thermal expansion\n",
      "- Warming oceans\n",
      "- Melting of land-based ice\n",
      "- Glaciers\n",
      "- Ice sheets\n",
      "- Greenland\n",
      "- Antarctica\n",
      "- 20th century\n",
      "- 2006-2015\n",
      "- Coastal flooding\n",
      "- Erosion of coastlines\n",
      "- Saltwater intrusion\n",
      "- Freshwater aquifers\n",
      "- Millions of people\n",
      "- Low-lying coastal areas\n",
      "- Displacement\n",
      "- Island nations\n",
      "- Maldives\n",
      "- Critical infrastructure\n",
      "- Ecosystems\n",
      "- Wetlands\n",
      "- Mangrove forests\n",
      "- Storms\n",
      "- Species\n",
      "\n",
      "### Entities in Retrieved Context (CE):\n",
      "- Global mean sea level\n",
      "- 1880\n",
      "- Last two and a half decades\n",
      "- Thermal expansion\n",
      "- Warming of the ocean\n",
      "- Increased melting\n",
      "- Land-based ice\n",
      "- Glaciers\n",
      "- Ice sheets\n",
      "- Oceans\n",
      "- Atmospheric heat\n",
      "- Emissions\n",
      "- Human activity\n",
      "- 2100\n",
      "- Coastal communities\n",
      "- Infrastructure\n",
      "- Ecosystems\n",
      "- Saltwater intrusion\n",
      "- Freshwater aquifers\n",
      "- Drinking water sources\n",
      "- Agricultural production\n",
      "- Coastal areas\n",
      "- Intergovernmental Panel on Climate Change (IPCC)\n",
      "- Climate change\n",
      "- Coastal flooding\n",
      "- Low-lying island nations\n",
      "- Maldives\n",
      "- Marshall Islands\n",
      "- Wetlands\n",
      "- Mangrove forests\n",
      "- Coastlines\n",
      "- Storms\n",
      "- Flooding\n",
      "- Local land subsidence\n",
      "- Ocean currents\n",
      "- Variations in land height\n",
      "\n",
      "### Common Entities (GE ∩ CE):\n",
      "- Climate change\n",
      "- Thermal expansion\n",
      "- Warming of the ocean (related to warming oceans)\n",
      "- Land-based ice\n",
      "- Glaciers\n",
      "- Ice sheets\n",
      "- Coastal flooding\n",
      "- Saltwater intrusion\n",
      "- Freshwater aquifers\n",
      "- Ecosystems\n",
      "- Wetlands\n",
      "- Mangrove forests\n",
      "- Storms\n",
      "- Maldives\n",
      "\n",
      "### Calculation:\n",
      "- |GE| (Total entities in ground truth): 26\n",
      "- |GE ∩ CE| (Common entities): 14\n",
      "- Context Entities Recall = |GE ∩ CE| / |GE| = 14 / 26\n",
      "\n",
      "### Final Context Entities Recall Score:\n",
      "- Context Entities Recall = 0.5385\n",
      "\n",
      "### Interpretation:\n",
      "The Context Entities Recall score of approximately 0.54 indicates that just over half of the entities mentioned in the ground truth are also present in the retrieved context. This suggests a moderate level of recall, indicating that while the retrieved context covers a significant portion of the entities from the ground truth, there are still numerous specific entities (especially locations and some specific impacts) that are not captured in the retrieved context. This could imply that while the context is somewhat comprehensive, it may miss certain details crucial for a full understanding or representation of the ground truth's content.\n"
     ]
    }
   ],
   "source": [
    "context_entities_recall_system_prompt = \"You are an AI assistant specialized in evaluating the recall of entities in retrieved context compared to ground truth. Your task is to identify entities in both the ground truth and context, compare these sets, and compute a Context Entities Recall score. Use your understanding of entity recognition to make thorough identifications and perform accurate calculations.\"\n",
    "\n",
    "context_entities_recall_response = generate_response(\n",
    "    context_entities_recall_system_prompt, context_entities_recall_prompt\n",
    ")\n",
    "\n",
    "print(context_entities_recall_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Semantic Similarity\n",
    "\n",
    "Answer Semantic Similarity is a metric that evaluates how closely the meaning of the generated answer aligns with the ground truth or expected answer. This metric goes beyond exact word matching to assess the overall semantic correspondence between the two answers.\n",
    "\n",
    "Key aspects of Answer Semantic Similarity calculation include:\n",
    "\n",
    "1. Embedding Generation:\n",
    "   - Convert both the generated answer and the ground truth answer into dense vector representations (embeddings) using pre-trained language models.\n",
    "\n",
    "2. Similarity Computation:\n",
    "   - Calculate the similarity between the two embeddings, often using cosine similarity or other vector similarity measures.\n",
    "\n",
    "3. Score Interpretation:\n",
    "   - The resulting similarity score typically ranges from 0 to 1, where 1 indicates perfect semantic similarity and 0 indicates no semantic overlap.\n",
    "\n",
    "4. Thresholding:\n",
    "   - Determine acceptable levels of similarity based on the specific requirements of your application.\n",
    "\n",
    "This metric is particularly useful because it can capture the correctness of an answer even when it's phrased differently from the ground truth. It allows for a more nuanced evaluation of the RAG system's ability to generate contextually and semantically appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Semantic Similarity Calculation Prompt\n",
    "\n",
    "answer_semantic_similarity_prompt = f\"\"\"\n",
    "You are an AI assistant tasked with evaluating the semantic similarity between a generated answer and a ground truth answer. Your goal is to calculate an Answer Semantic Similarity score using the following process:\n",
    "\n",
    "1. Given:\n",
    "   - Ground Truth Answer: {ground_truth}\n",
    "   - Generated Answer: {answer}\n",
    "\n",
    "2. Task Overview:\n",
    "   Your task is to compare the semantic meaning of the generated answer to the ground truth answer and assign a similarity score between 0 and 1, where 1 indicates perfect semantic similarity and 0 indicates no semantic similarity.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "   a. Content Analysis:\n",
    "      - Identify the main concepts, facts, and arguments present in both the ground truth and generated answer.\n",
    "      - List these key elements for each answer.\n",
    "\n",
    "   b. Structural Comparison:\n",
    "      - Compare the organization and flow of ideas between the two answers.\n",
    "      - Note any significant differences or similarities in structure.\n",
    "\n",
    "   c. Semantic Evaluation:\n",
    "      - Assess how well the generated answer captures the meaning and intent of the ground truth answer.\n",
    "      - Consider factors such as:\n",
    "        * Accuracy of information\n",
    "        * Completeness of the response\n",
    "        * Relevance of the content\n",
    "        * Consistency in terminology and concepts\n",
    "        * Depth of explanation\n",
    "\n",
    "   d. Assign Similarity Score:\n",
    "      - Based on your analysis, assign a similarity score between 0 and 1.\n",
    "      - Provide a detailed justification for your score, referencing specific aspects of your analysis.\n",
    "\n",
    "4. Final Output:\n",
    "   - List the key elements identified in both answers\n",
    "   - Summarize your structural and semantic comparison\n",
    "   - Present the final Answer Semantic Similarity score\n",
    "   - Provide a detailed explanation of how you arrived at this score\n",
    "\n",
    "Remember to focus on the semantic meaning rather than exact wording, and be as objective as possible in your assessment.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "Ground Truth Key Elements:\n",
    "[List of key elements]\n",
    "\n",
    "Generated Answer Key Elements:\n",
    "[List of key elements]\n",
    "\n",
    "Structural Comparison:\n",
    "[Summary of structural similarities and differences]\n",
    "\n",
    "Semantic Evaluation:\n",
    "[Detailed analysis of semantic similarity, addressing the factors mentioned]\n",
    "\n",
    "Answer Semantic Similarity Score: [Score between 0 and 1]\n",
    "\n",
    "Justification:\n",
    "[Detailed explanation of the score, referencing specific aspects of the analysis]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Ground Truth Key Elements:\n",
      "1. Climate change is causing global sea levels to rise at an accelerating rate.\n",
      "2. Primary factors: thermal expansion of warming oceans and melting of land-based ice, particularly glaciers and ice sheets in Greenland and Antarctica.\n",
      "3. Rate of sea-level rise increased from 1.4 mm per year in the 20th century to 3.6 mm per year from 2006-2015.\n",
      "4. Consequences include increased coastal flooding, erosion of coastlines, and saltwater intrusion into freshwater aquifers.\n",
      "5. Millions of people in low-lying coastal areas are at risk of displacement.\n",
      "6. Some island nations, like the Maldives, are at risk of becoming completely submerged.\n",
      "7. Rising seas can damage critical infrastructure and ecosystems, such as wetlands and mangrove forests.\n",
      "8. Wetlands and mangrove forests provide natural protection against storms and serve as important habitats for many species.\n",
      "\n",
      "### Generated Answer Key Elements:\n",
      "1. Climate change is causing global sea levels to rise at an accelerating rate.\n",
      "2. Primary factors: thermal expansion of the oceans as they warm and the melting of land-based ice, particularly glaciers and ice sheets in Greenland and Antarctica.\n",
      "3. Rate of sea-level rise increased from 1.4 mm per year in the 20th century to 3.6 mm per year from 2006-2015.\n",
      "4. Consequences include increased coastal flooding, erosion of coastlines, saltwater intrusion into freshwater aquifers, and potential displacement of millions in low-lying coastal areas.\n",
      "5. Some island nations, like the Maldives, are at risk of becoming completely submerged.\n",
      "6. Rising seas can damage critical infrastructure and ecosystems, such as wetlands and mangrove forests.\n",
      "7. Wetlands and mangrove forests provide natural protection against storms and serve as important habitats for many species.\n",
      "\n",
      "### Structural Comparison:\n",
      "Both answers are structured similarly, presenting the cause of sea level rise, followed by its effects and specific risks to ecosystems and human populations. The generated answer condenses some information but maintains a logical flow that closely mirrors the ground truth.\n",
      "\n",
      "### Semantic Evaluation:\n",
      "- **Accuracy of Information**: Both answers accurately describe the causes and effects of sea level rise due to climate change.\n",
      "- **Completeness of the Response**: The generated answer covers all major points from the ground truth but combines some points into more general statements, slightly reducing detail.\n",
      "- **Relevance of the Content**: All information in the generated answer is relevant and closely aligns with the ground truth content.\n",
      "- **Consistency in Terminology and Concepts**: Both answers use similar terminology and concepts, with minor variations in phrasing that do not alter the meaning.\n",
      "- **Depth of Explanation**: The generated answer provides a succinct yet comprehensive explanation, similar in depth to the ground truth.\n",
      "\n",
      "### Answer Semantic Similarity Score: 0.95\n",
      "\n",
      "### Justification:\n",
      "The generated answer closely matches the ground truth in terms of content, structure, and semantics. It effectively captures all the key elements, albeit with slightly less detail in describing the consequences and risks associated with sea level rise. The minor condensation of some points results in a very high but not perfect similarity score. The score of 0.95 reflects the high degree of semantic similarity, acknowledging the slight reduction in detail which does not significantly impact the overall accuracy or relevance of the information provided.\n"
     ]
    }
   ],
   "source": [
    "answer_semantic_similarity_system_prompt = \"You are an AI assistant specialized in evaluating the semantic similarity between generated answers and ground truth answers. Your task is to analyze the content, structure, and meaning of both answers, and compute an Answer Semantic Similarity score. Use your understanding of language and semantics to make thorough comparisons and provide a justified similarity score.\"\n",
    "\n",
    "answer_semantic_similarity_response = generate_response(\n",
    "    answer_semantic_similarity_system_prompt, answer_semantic_similarity_prompt\n",
    ")\n",
    "\n",
    "print(answer_semantic_similarity_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Correctness\n",
    "\n",
    "Answer Correctness is a fundamental metric in evaluating the performance of Retrieval-Augmented Generation (RAG) systems. It assesses the accuracy and factual consistency of the generated answers with respect to the provided context and the ground truth.\n",
    "\n",
    "Key aspects of Answer Correctness evaluation include:\n",
    "\n",
    "1. Factual Accuracy:\n",
    "   - Verifying that the information presented in the generated answer aligns with the facts in the source documents and ground truth.\n",
    "\n",
    "2. Completeness:\n",
    "   - Ensuring that the generated answer covers all relevant aspects of the question without omitting crucial information.\n",
    "\n",
    "3. Relevance:\n",
    "   - Checking that the answer directly addresses the question asked and doesn't include extraneous or off-topic information.\n",
    "\n",
    "4. Consistency:\n",
    "   - Confirming that the answer maintains internal consistency and doesn't contradict itself or the known facts.\n",
    "\n",
    "This metric is crucial because it directly impacts the reliability and trustworthiness of the RAG system. A high level of answer correctness indicates that the system is effectively retrieving relevant information and generating accurate responses based on that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Correctness Calculation Prompt\n",
    "\n",
    "answer_correctness_prompt = f\"\"\"\n",
    "You are an AI assistant tasked with evaluating the correctness of a generated answer compared to a ground truth answer. Your goal is to calculate an Answer Correctness score using the following process:\n",
    "\n",
    "1. Given:\n",
    "   - Ground Truth Answer: {ground_truth}\n",
    "   - Generated Answer: {answer}\n",
    "   - Semantic Weight: 0.5\n",
    "   - Factual Weight: 0.5\n",
    "   - Threshold (optional): None\n",
    "\n",
    "2. Task Overview:\n",
    "   Your task is to evaluate both the semantic and factual similarity between the generated answer and the ground truth, combine these scores using the given weights, and calculate an overall Answer Correctness score.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "   a. Semantic Similarity Evaluation:\n",
    "      - Assess how well the generated answer captures the meaning and intent of the ground truth answer.\n",
    "      - Consider factors such as:\n",
    "        * Consistency in terminology and concepts\n",
    "        * Completeness of the response\n",
    "        * Depth of explanation\n",
    "      - Assign a semantic similarity score between 0 and 1.\n",
    "      - Briefly justify your semantic similarity score.\n",
    "\n",
    "   b. Factual Similarity Evaluation:\n",
    "      - Identify key facts, figures, and claims in both answers.\n",
    "      - Compare the accuracy of these elements between the generated answer and ground truth.\n",
    "      - Consider factors such as:\n",
    "        * Correctness of specific data points\n",
    "        * Accuracy of statements and claims\n",
    "        * Presence of all crucial facts from the ground truth\n",
    "      - Assign a factual similarity score between 0 and 1.\n",
    "      - Briefly justify your factual similarity score.\n",
    "\n",
    "   c. Calculate Weighted Answer Correctness Score:\n",
    "      - Use the formula: \n",
    "        Answer Correctness = (Semantic Weight * Semantic Score) + (Factual Weight * Factual Score)\n",
    "      - Show your calculation.\n",
    "\n",
    "   d. Apply Threshold (if provided):\n",
    "      - If a threshold is given, convert the score to binary:\n",
    "        * If Answer Correctness >= Threshold, set score to 1\n",
    "        * If Answer Correctness < Threshold, set score to 0\n",
    "\n",
    "4. Final Output:\n",
    "   - Present the Semantic Similarity score with justification\n",
    "   - Present the Factual Similarity score with justification\n",
    "   - Show the calculation of the weighted Answer Correctness score\n",
    "   - If applicable, show the binary threshold conversion\n",
    "   - Provide the final Answer Correctness score\n",
    "   - Give a brief interpretation of what the score means for the answer's correctness\n",
    "\n",
    "Remember to be as objective as possible in your assessment and provide clear justifications for your scores.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "Semantic Similarity Score: [Score between 0 and 1]\n",
    "Justification: [Brief explanation]\n",
    "\n",
    "Factual Similarity Score: [Score between 0 and 1]\n",
    "Justification: [Brief explanation]\n",
    "\n",
    "Weighted Answer Correctness Calculation:\n",
    "([Semantic Weight] * [Semantic Score]) + ([Factual Weight] * [Factual Score]) = [Weighted Score]\n",
    "\n",
    "[If applicable] Threshold Application:\n",
    "Original Score: [Weighted Score]\n",
    "Threshold: [Threshold Value]\n",
    "Binary Score: [0 or 1]\n",
    "\n",
    "Final Answer Correctness Score: [Final Score]\n",
    "\n",
    "Interpretation:\n",
    "[Brief interpretation of the score and what it means for the answer's correctness]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Semantic Similarity Score: 0.95\n",
      "**Justification:** The generated answer closely captures the meaning and intent of the ground truth answer. It maintains consistency in terminology and concepts, such as \"thermal expansion,\" \"melting of land-based ice,\" and \"sea-level rise.\" The response is comprehensive, covering all major points from the ground truth, including the consequences of rising sea levels and the risks to island nations and ecosystems. The depth of explanation is also well-aligned with the ground truth, providing a detailed overview of the causes and effects of sea level rise.\n",
      "\n",
      "### Factual Similarity Score: 0.95\n",
      "**Justification:** The generated answer accurately reflects the key facts and figures presented in the ground truth. It correctly cites the rate of sea-level rise from the 20th century and from 2006-2015, and it identifies the primary causes of sea level rise. The answer also correctly lists the consequences of rising sea levels, such as coastal flooding, erosion, and the risk to low-lying areas and island nations. All crucial facts from the ground truth are present and accurately represented.\n",
      "\n",
      "### Weighted Answer Correctness Calculation:\n",
      "(0.5 * 0.95) + (0.5 * 0.95) = 0.95\n",
      "\n",
      "### Final Answer Correctness Score: 0.95\n",
      "\n",
      "**Interpretation:**\n",
      "The final score of 0.95 indicates a high level of correctness in the generated answer compared to the ground truth. This score reflects both the semantic and factual accuracy of the generated answer, demonstrating that it effectively and accurately conveys the information about the impact of climate change on sea levels, as outlined in the ground truth. The generated answer is reliable and provides a comprehensive understanding of the topic.\n"
     ]
    }
   ],
   "source": [
    "answer_correctness_system_prompt = \"You are an AI assistant specialized in evaluating the correctness of generated answers compared to ground truth answers. Your task is to assess both semantic and factual similarity, combine these assessments into a weighted score, and optionally apply a threshold for binary classification. Use your understanding of language and facts to make thorough comparisons and provide justified scores.\"\n",
    "\n",
    "answer_correctness_response = generate_response(\n",
    "    answer_correctness_system_prompt, answer_correctness_prompt\n",
    ")\n",
    "\n",
    "print(answer_correctness_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's display the evaluation result in a streamlit app\n",
    "\n",
    "*Note*: streamlit may not working in Kaggle notebook, you can run the code in your local machine or use the Streamlit sharing service to deploy the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def display_results(results: List[Dict[str, any]]):\n",
    "    \"\"\"Display results in a Streamlit app with improved readability for long text.\"\"\"\n",
    "    st.title(\"Evaluation Results\")\n",
    "\n",
    "    if not results:\n",
    "        st.info(\"No results to display.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Display each result in an expander\n",
    "    for index, row in df.iterrows():\n",
    "        with st.expander(f\"Result {index + 1}\"):\n",
    "            for column, value in row.items():\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    st.subheader(column)\n",
    "                    st.text_area(\"\", value, height=150)\n",
    "                else:\n",
    "                    st.subheader(column)\n",
    "                    st.write(value)\n",
    "\n",
    "    # Export to CSV\n",
    "    csv = df.to_csv(index=False).encode(\"utf-8\")\n",
    "    st.download_button(\n",
    "        label=\"Download results as CSV\",\n",
    "        data=csv,\n",
    "        file_name=\"evaluation_results.csv\",\n",
    "        mime=\"text/csv\",\n",
    "    )\n",
    "\n",
    "    # Display statistics\n",
    "    st.subheader(\"Statistics\")\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    if not numeric_columns.empty:\n",
    "        stats_df = df[numeric_columns].describe()\n",
    "        st.dataframe(stats_df)\n",
    "\n",
    "        # Visualizations\n",
    "        st.subheader(\"Visualizations\")\n",
    "        for column in numeric_columns:\n",
    "            st.write(f\"Distribution of {column}\")\n",
    "            st.histogram(df[column])\n",
    "    else:\n",
    "        st.info(\"No numeric data available for statistics and visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    {\n",
    "        \"Query\": query,\n",
    "        \"Answer\": answer,\n",
    "        \"Context\": context,\n",
    "        \"Faithfulness\": faithfulness_response,\n",
    "        \"Answer Correctness\": answer_correctness_response,\n",
    "        \"Context Precision\": context_precision_response,\n",
    "        \"Context Relevancy\": context_relevance_response,\n",
    "        \"Context Recall\": context_recall_response,\n",
    "        \"Context Entities Recall\": context_entities_recall_response,\n",
    "        \"Answer Semantic Similarity\": answer_semantic_similarity_response,\n",
    "        \"Answer Correctness\": answer_correctness_response,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run the streamlit app, got some error, need to fix it\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored a comprehensive approach to evaluating Retrieval-Augmented Generation (RAG) systems, moving beyond intuition to implement systematic, quantifiable metrics. By focusing on key aspects such as Faithfulness, Relevance, Context Precision, Context Recall, and Context Entities Recall, we've established a robust framework for assessing RAG performance.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Objective Measurement**: We've implemented metrics that provide concrete, comparable data on our RAG system's performance, enabling informed decision-making in the development process.\n",
    "\n",
    "2. **LLM-as-Judge Approach**: While leveraging LLMs for evaluation offers efficiency at scale, we've acknowledged and addressed potential limitations through strategies like detailed criteria, example-based instruction, and human oversight.\n",
    "\n",
    "3. **Streamlit Visualization**: By integrating our evaluation metrics into a Streamlit application, we've created an interactive, user-friendly interface for analyzing and presenting results, making it easier to identify trends and areas for improvement.\n",
    "\n",
    "4. **Holistic Evaluation**: Our approach considers multiple dimensions of RAG performance, from the accuracy of retrieved context to the relevance of generated responses, providing a comprehensive view of system capabilities.\n",
    "\n",
    "5. **Continuous Improvement**: With these evaluation tools in place, we're well-equipped to iteratively refine our RAG system, making data-driven decisions to enhance its effectiveness.\n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "As you continue to develop and refine your RAG systems, remember that evaluation is an ongoing process. Regular assessment using these metrics will help you:\n",
    "\n",
    "- Track progress over time\n",
    "- Compare different system configurations\n",
    "- Identify specific areas for optimization\n",
    "- Communicate improvements to stakeholders more effectively\n",
    "\n",
    "While this evaluation framework provides a solid foundation, don't hesitate to adapt and expand upon it as your specific use cases evolve. The field of RAG is rapidly advancing, and staying flexible in your evaluation approach will be key to long-term success.\n",
    "\n",
    "By combining systematic evaluation with intuitive visualization, you're now well-positioned to build more accurate, relevant, and effective RAG systems. Keep iterating, keep measuring, and keep pushing the boundaries of what's possible with retrieval-augmented generation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
