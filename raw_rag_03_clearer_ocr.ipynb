{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw RAG 03: OCR, Much Clearer\n",
    "\n",
    "In today's business landscape, many enterprises still rely heavily on paper documents. These documents, often scanned and stored as images, contain crucial information that needs to be extracted and digitized. While Optical Character Recognition (OCR) is the traditional solution for this task, even advanced cloud-based OCR services often fall short when dealing with complex, real-world documents.\n",
    "\n",
    "## The Limitations of Existing OCR Solutions\n",
    "\n",
    "Despite technological advancements, OCR remains challenging:\n",
    "\n",
    "- Traditional OCR struggles with handwritten text and non-standard formats\n",
    "- Even cloud OCR services from tech giants like AWS and Google Cloud have limitations:\n",
    "  - Constraints on document formats\n",
    "  - Difficulty with unconventional layouts\n",
    "  - Inconsistent performance across different document types\n",
    "- Many solutions miss the mark when it comes to real-world, diverse business documents\n",
    "\n",
    "These shortcomings highlight the need for more robust, flexible approaches to document preprocessing.\n",
    "\n",
    "## Innovative Approaches\n",
    "\n",
    "This notebook introduces three progressive techniques to significantly improve OCR results:\n",
    "\n",
    "1. **Local OCR Implementation**: We'll start with a basic, locally-run OCR system. This gives us full control over the initial text extraction process.\n",
    "\n",
    "2. **LLM-Based Correction and Enhancement**: We'll use Large Language Models (LLMs) to correct and refine the output from our local OCR. This combination has yielded surprisingly good results, especially for challenging documents.\n",
    "\n",
    "3. **Multi-Modal LLM for Direct OCR**: Leveraging the latest advancements in AI, we'll explore using multi-modal LLMs to perform OCR directly on document images. This cutting-edge approach can potentially overcome many limitations of traditional OCR methods.\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "\n",
    "1. Setting up and running a local OCR system\n",
    "2. Implementing LLM-based correction of OCR output\n",
    "3. Utilizing multi-modal LLMs for direct image-to-text conversion\n",
    "4. Analyzing and comparing the results of each approach\n",
    "5. Discussing the pros and cons of each method for different document types\n",
    "\n",
    "By exploring these three approaches - from traditional local OCR to advanced AI-driven techniques - we aim to provide a comprehensive overview of modern document preprocessing methods. This exploration will equip you with the knowledge to choose the best approach for your specific document understanding needs.\n",
    "\n",
    "Let's dive in and discover how we can push the boundaries of OCR technology, addressing the limitations of both traditional and cloud-based solutions while exploring the exciting possibilities of multi-modal AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install local ocr dependencies\n",
    "\n",
    "%pip install PyPDF2 pytesseract pillow python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = \".env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "  \"\"\"\n",
    "  Extracts text from a PDF file.\n",
    "\n",
    "  Args:\n",
    "    pdf_path (str): The path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "    str: The extracted text from the PDF file.\n",
    "  \"\"\"\n",
    "  # Open the PDF file\n",
    "  with open(pdf_path, \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to store all the text\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page in reader.pages:\n",
    "      # Try to extract text directly first\n",
    "      text = page.extract_text()\n",
    "\n",
    "      # If no text was extracted, it might be an image-based PDF\n",
    "      if not text:\n",
    "        # Extract images from the page\n",
    "        for image in page.images:\n",
    "          # Open the image using PIL\n",
    "          img = Image.open(io.BytesIO(image.data))\n",
    "\n",
    "          # Use Tesseract to do OCR on the image\n",
    "          text = pytesseract.image_to_string(img)\n",
    "\n",
    "          # Add the extracted text to our full text\n",
    "          full_text += text + \"\\n\"\n",
    "      else:\n",
    "        # If text was extracted directly, add it to our full text\n",
    "        full_text += text + \"\\n\"\n",
    "\n",
    "  return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a US Supreme Court Opinion PDF\n",
    "pdf_path = \"docs/service-ll-usrep-usrep001-usrep001410a-usrep001410a.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform local OCR text extraction\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the PDF file\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(pdf_path, width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon comparing the original PDF with the extracted text, it becomes evident that our OCR output, while functional, is far from perfect. The results are riddled with various issues:\n",
    "\n",
    "- Numerous spelling mistakes\n",
    "- Misinterpreted characters\n",
    "- Formatting inconsistencies\n",
    "- Potential loss of critical information\n",
    "\n",
    "These imperfections underscore the limitations of traditional OCR techniques, especially when dealing with complex or low-quality documents. However, this is where our innovative approach comes into play.\n",
    "\n",
    "In the next exciting step of our process, we'll harness the power of Large Language Models (LLMs) to address these shortcomings. By leveraging LLMs, we aim to:\n",
    "\n",
    "1. Correct spelling and grammatical errors\n",
    "2. Reconstruct misinterpreted words and phrases\n",
    "3. Infer missing or unclear information based on context\n",
    "4. Enhance the overall quality and readability of the extracted text\n",
    "\n",
    "This LLM-driven correction phase represents a significant leap forward in OCR technology, potentially transforming imperfect extractions into highly accurate and usable text. Let's dive in and explore how LLMs can elevate our OCR results to new heights of accuracy and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance OCR output using LLM-based text refinement\n",
    "#\n",
    "# This step leverages a Large Language Model to:\n",
    "#   1. Correct OCR-induced errors (e.g., misspellings, misinterpretations)\n",
    "#   2. Improve text coherence and readability\n",
    "#   3. Reconstruct ambiguous or partially extracted content\n",
    "#\n",
    "# The LLM acts as an intelligent post-processing layer, significantly\n",
    "# elevating the quality of our extracted text beyond standard OCR capabilities.\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "full_query = f\"\"\"Below is a scanned PDF document, please attempt to clean up the text and return it formatted. Don't add any text pre or post the extracted text.\n",
    "\n",
    "Original text:\n",
    "\\\"\\\"\\\"\n",
    "{extracted_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You help user with their request.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": full_query},\n",
    "    ],\n",
    "    model=\"gpt-4-turbo\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "corrected_text = response.choices[0].message.content\n",
    "\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM-enhanced result demonstrates a marked improvement over the original OCR output. We observe:\n",
    "\n",
    "- Significant reduction in spelling and grammatical errors\n",
    "- Enhanced coherence and readability of the extracted text\n",
    "- Better preservation of the document's original meaning and context\n",
    "\n",
    "This substantial enhancement underscores the power of combining traditional OCR with LLM-based refinement. However, our exploration doesn't stop here.\n",
    "\n",
    "Next, we'll push the boundaries further by leveraging a state-of-the-art vision model to extract text directly from the PDF. This cutting-edge approach bypasses traditional OCR entirely, potentially offering:\n",
    "\n",
    "1. Improved handling of complex layouts and non-standard fonts\n",
    "2. Better interpretation of visual context and document structure\n",
    "3. Possibly higher accuracy in challenging scenarios (e.g., handwritten text, low-quality scans)\n",
    "\n",
    "By comparing the results of this vision model-based extraction with our LLM-enhanced OCR output, we'll gain valuable insights into the strengths and limitations of each approach. This comparison will help us determine the most effective method for various document types and quality levels.\n",
    "\n",
    "Let's proceed with this exciting next step and see how the vision model performs in our document extraction task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies\n",
    "\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF first page to base64 string\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "def pdf_first_page_to_base64(pdf_path: str) -> str:\n",
    "  \"\"\"\n",
    "  Converts the first page of a PDF file to a base64 encoded string.\n",
    "\n",
    "  Args:\n",
    "    pdf_path (str): The path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "    str: The base64 encoded string representation of the first page of the PDF.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Open the PDF file\n",
    "  pdf_document = fitz.open(pdf_path)\n",
    "  \n",
    "  # Get the first page\n",
    "  first_page = pdf_document[0]\n",
    "  \n",
    "  # Convert the page to an image\n",
    "  pix = first_page.get_pixmap()\n",
    "  \n",
    "  # Convert pixmap to PIL Image\n",
    "  img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "  \n",
    "  # Create a byte stream\n",
    "  img_byte_arr = io.BytesIO()\n",
    "  \n",
    "  # Save the image as PNG to the byte stream\n",
    "  img.save(img_byte_arr, format='PNG')\n",
    "  \n",
    "  # Get the byte string\n",
    "  img_byte_arr = img_byte_arr.getvalue()\n",
    "  \n",
    "  # Encode to base64\n",
    "  base64_encoded = base64.b64encode(img_byte_arr).decode('utf-8')\n",
    "  \n",
    "  # Close the PDF document\n",
    "  pdf_document.close()\n",
    "  \n",
    "  return base64_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the first page of the PDF to a base64 string\n",
    "\n",
    "base64_string = pdf_first_page_to_base64(pdf_path)\n",
    "print(base64_string[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the image using OpenAI mutli-modal model (GPT-4o)\n",
    "\n",
    "img_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"extract text from the image, return it formatted. Do not add any additional text pre or post.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_string}\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "base64_resp = img_response.choices[0].message.content\n",
    "\n",
    "print(base64_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markdown_table(col1, col2, col3, text1, text2, text3):\n",
    "    table = f\"\"\"\n",
    "| {col1} | {col2} | {col3} |\n",
    "|----------|----------|----------|\n",
    "| {text1} | {text2} | {text3} |\n",
    "\"\"\"\n",
    "\n",
    "    return table.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_text = create_markdown_table(\n",
    "    \"Local OCR Extraction\",\n",
    "    \"GPT4 Formatted (Base on Local OCR)\",\n",
    "    \"GPT4o Vision Extraction\",\n",
    "    extracted_text,\n",
    "    corrected_text, base64_resp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the comparison table\n",
    "\n",
    "print(compare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "OCR is not enough for non-standardized text. It can be beneficial to use LLM to correct the OCR output. But due to the volume of the data, it may not be fesible to use LLM for all the documents. Local models can be a substitute for cloud services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
